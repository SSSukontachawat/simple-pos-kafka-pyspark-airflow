# Simple POS - Kafka, PySpark, Airflow

## ğŸš€ Overview
This project is a **simple POS (Point-of-Sale) system** using **Kafka, PySpark, Airflow, and PostgreSQL**, running in Docker.

## ğŸ—ï¸ Architecture
1. **Kafka Producer (Streamlit App)**
   - Allows users to **add, edit, and delete transactions, products, and members**.
   - Publishes data to Kafka topics.

2. **Kafka Consumer**
   - Reads Kafka messages and stores them in **CSV files**.

3. **PySpark & Airflow**
   - **Processes CSV data** and loads it into **PostgreSQL**.
   - **Hourly:** Updates recent data.
   - **Daily:** Uploads all transactions at 10 PM.

4. **Docker Compose**
   - Manages all services in containers.

## Structure
simple-pos-kafka-pyspark-airflow  
â”œâ”€â”€ secondDataPipeline/  
â”‚   â”œâ”€â”€ kafka_producer/  
â”‚   â”‚   â”œâ”€â”€ Dockerfile  
â”‚   â”‚   â”œâ”€â”€ requirements.txt  
â”‚   â”‚   â””â”€â”€ kafka_producer.py  
â”‚   â”œâ”€â”€ kafka_consumer/  
â”‚   â”‚   â”œâ”€â”€ Dockerfile  
â”‚   â”‚   â”œâ”€â”€ requirements.txt  
â”‚   â”‚   â””â”€â”€ kafka_consumer.py  
â”œâ”€â”€ airflow/  
â”‚   â”œâ”€â”€ Dockerfile  
â”‚   â”œâ”€â”€ requirements.txt  
â”‚   â””â”€â”€ dags/  
â”‚       â””â”€â”€ POS_spark_dag.py  
â”œâ”€â”€ docker-compose.yml  
â””â”€â”€ .github/  
    â””â”€â”€ workflows/  
        â””â”€â”€ ci-cd.yml  
        
## ğŸ› ï¸ How to Run
### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/SSSukontachawat/simple-pos-kafka-pyspark-airflow.git
cd simple-pos-kafka-pyspark-airflow
```
### **2ï¸âƒ£ Run Docker
```
docker-compose up -d --build
```
### **3ï¸âƒ£ Access Services
Streamlit App (Producer UI): http://localhost:8501
Airflow UI: http://localhost:8080
```
docker-compose down
```

## ğŸ† Key Features
âœ… Real-time data streaming with Kafka
âœ… Data processing with PySpark
âœ… Task scheduling with Airflow
âœ… CI/CD automation with GitHub Actions

## ğŸ“œ License
MIT License.
