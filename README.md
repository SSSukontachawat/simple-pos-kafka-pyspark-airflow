# Simple POS - Kafka, PySpark, Airflow

## ğŸš€ Overview
This project is a **simple POS (Point-of-Sale) system** using **Kafka, PySpark, Airflow, and PostgreSQL**, running in Docker.

## ğŸ—ï¸ Architecture
1. **Kafka Producer (Streamlit App)**
   - Allows users to **add, edit, and delete transactions, products, and members**.
   - Publishes data to Kafka topics.

2. **Kafka Consumer**
   - Reads Kafka messages and stores them in **CSV files**.

3. **PySpark & Airflow**
   - **Processes CSV data** and loads it into **PostgreSQL**.
   - **Hourly:** Updates recent data.
   - **Daily:** Uploads all transactions at 10 PM.

4. **Docker Compose**
   - Manages all services in containers.

## ğŸ› ï¸ How to Run
### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/SSSukontachawat/simple-pos-kafka-pyspark-airflow.git
cd simple-pos-kafka-pyspark-airflow
```
### **2ï¸âƒ£ Run Docker
```
docker-compose up -d --build
```
### **3ï¸âƒ£ Access Services
Streamlit App (Producer UI): http://localhost:8501
Airflow UI: http://localhost:8080
```
docker-compose down
```

## ğŸ† Key Features
âœ… Real-time data streaming with Kafka
âœ… Data processing with PySpark
âœ… Task scheduling with Airflow
âœ… CI/CD automation with GitHub Actions

## ğŸ“œ License
MIT License.
